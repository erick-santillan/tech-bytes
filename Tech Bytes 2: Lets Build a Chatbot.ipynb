{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Bytes 2: Lets build a chatbot!\n",
    "\n",
    "Today, we're going to learn how to build a chatbot in python. We won't have time to do a new UI but we will build some functions that may allow you to call openAI from python :)\n",
    "Let's remember a little of what we saw last time.\n",
    "\n",
    "Last time we talked about requests, and how they are the backbone of internet communication. ChatGPT is no different than this, so the way we interact with it is by sending a request to a specific endpoint. There are however some differences, we need to send some authentication as well as some parameters that influence the response of the model.\n",
    "\n",
    "But let's start with importing the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://oai-tech-bytes.openai.azure.com/\"\n",
    "URL = f\"{BASE_URL}openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview\"\n",
    "AZURE_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"messages\": [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a joke about mango\",\n",
    "    }],\n",
    "\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": AZURE_KEY,\n",
    "}\n",
    "\n",
    "resp = requests.post(URL, headers=headers, json=payload)\n",
    "\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there's a lot of information in the response. The most important part is the choices key, which contains the response from the model. \n",
    "The response is a list of messages, with each message containing the role (user or AI) and the content (the text of the message).\n",
    "To extract the messages we do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using OpenAI library\n",
    "OpenAI has a library that allows us to call the model in a little bit of more convenient or `pythonic` way. Here is how we can use the OpenAI library to call the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_KEY,  \n",
    "    api_version=\"2024-08-01-preview\",\n",
    "    azure_endpoint = BASE_URL\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a joke about mango\",\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = response.choices[0].message.content\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What else can you do?\n",
    "The API not only accepts messages, but also other inputs such as temperature, max_tokens, etc... You can find the full list [here](https://platform.openai.com/docs/api-reference/chat/create). Let's play around with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tempterature\n",
    "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"If the the Mona Lisa was alive today, which social media platform would she use?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "    }],\n",
    "    temperature=0.9\n",
    ")\n",
    "\n",
    "reply = response.choices[0].message.content\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n : (Choices)\n",
    "How many completions to generate for each prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me a joke about engineers\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "    }],\n",
    "    n=3\n",
    ")\n",
    "\n",
    "for message in response.choices:\n",
    "    print(\"New choice\")\n",
    "    reply = message.message.content\n",
    "    print(reply)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logprobs\n",
    "Include the log probabilities on the logprobs most likely output tokens, as well the chosen tokens. For example, if logprobs is 5, the API will return a list of the five most likely tokens. The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Answer only with yes or no. Do you have a conscience?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "    }],\n",
    "    logprobs=True,\n",
    "    top_logprobs=3\n",
    ")\n",
    "\n",
    "reply = response.choices[0].message.content\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the different probablities of the model's response\n",
    "The probabiliy we get is the logprobs value, so we convert it to a percentage by taking the exponent of the logprobs value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_token = response.choices[0].logprobs.content[0]\n",
    "for probabilities in first_token.top_logprobs:\n",
    "    print(f\"{probabilities.token}: {math.exp(probabilities.logprob)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "\n",
    "There's way too many moving parts so far in this code snippet. Let's simplify it by creating a utility function that will handle the API call for us. You can modify it to take into consideration some of the parameters we saw before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chatgpt(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = query_chatgpt(\"Tell me a joke about mango\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT memory\n",
    "Let's do some experiments to see if GPT has memory. We will ask GPT to remember a fact and then ask a question about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = query_chatgpt(\"Remember: my favorite artist is Taylor Swift\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = query_chatgpt(\"Who is my favorite artist?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?? Did it forgot what we just talked about? No... It's simpler than that. It's just that are little pal is quite forgetful. It actually doesn't know anything about our previous conversations.\n",
    "\n",
    "But... How could we solve this you may ask? Well... essentially we need to send our entire conversation history to the model every time we want to chat with it. This way, it can keep track of the context and give us more coherent responses.\n",
    "\n",
    "There are smarter ways to do this, but for now, let's just keep it simple and send the entire conversation history to the model. This means that the longer the conversation, the more tokens we will use and the mroe likely the model is to get fixated into weird loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_CONVERSATION = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chatgpt_with_memory(prompt):\n",
    "    # We save the new message in our global conversation\n",
    "    GLOBAL_CONVERSATION.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=GLOBAL_CONVERSATION\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "    \n",
    "    # We save the new response in our global conversation as well!\n",
    "    GLOBAL_CONVERSATION.append(\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": reply,\n",
    "        }\n",
    "    )\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try again the previous examples with the new function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = query_chatgpt_with_memory(\"Remember: my favorite artist is Taylor Swift\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = query_chatgpt_with_memory(\"Who is my favorite artist?\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our memory,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in GLOBAL_CONVERSATION:\n",
    "    print(f\"{message['role']}: {message['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A toi de jouer!\n",
    "How can you start a new conversation? Can you imagine a smarter way to keep track of the memory?\n",
    "Does this explains some issues you have seen with chatGPT? How would you fix them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C'est fini!\n",
    "\n",
    "And that's it :) Hope you learned more about chatbots and how to use them with Azure OpenAI! If you have any questions, feel free to ask in the comments below. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
