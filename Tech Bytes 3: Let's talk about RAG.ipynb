{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Bytes 3: Let's talk about RAG\n",
    "\n",
    "Today, we're going to build a chatbot, but not any chatbot, we're going to build a chatbot that knows about stuff that you have teach it!\n",
    "\n",
    "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).\n",
    "\n",
    "A typical RAG application has two main components:\n",
    "\n",
    "* **Indexing**: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
    "\n",
    "* **Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-openai==0.1.7 langchain-core==0.2.1 langchain-community==0.2.1 langchain==0.2.1 sentence-transformers==3.0.0 faiss-cpu==1.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start with connecting to openAI\n",
    "This time we will use a library called `langchain` which makes easier the interactions between the LLM and python. So no need of requests.\n",
    "\n",
    "The main concept that you have to know is that in order to execute a call you have to do call the object with the method `.invoke()`. You will see an example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4o_mini = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4o-mini\",\n",
    "    api_key=api_key,\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    azure_endpoint=\"https://oai-tech-bytes.openai.azure.com/\"\n",
    "    )\n",
    "\n",
    "gpt4o_mini.invoke(\"What's the capital of Paris?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Ask gpt-4o what is AI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks a bit weird right? It's because langchain has some weird methods an objects attached to the response. We could parse the answer and get the actual text by using the `StrOutputParser()`\n",
    "\n",
    "We can use also the `|` or `pipe` operator and it essentially takes the output of of function and puts it as the input of the next function. So instead of doing:\n",
    "```python\n",
    "intermediate_output = gpt4o_mini.invoke(\"What's the capital of Paris?\")\n",
    "output = StrOutputParser().invoke(intermediate_output)\n",
    "```\n",
    "\n",
    "We do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = gpt4o_mini | StrOutputParser()\n",
    "llm.invoke(\"What's the capital of Paris?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "Ask again what is AI but suing the llm chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "You can also create prompt templates. These templates help to determined some placeholders and being able to reuse them without having to tap everytime the prompt.\n",
    "Let's generalize the prompt to be able to ask for the capital of any country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"What is the capital of {country}?\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a new chain for this\n",
    "\n",
    "capital_bot = prompt | gpt4o_mini | StrOutputParser()\n",
    "\n",
    "capital_bot.invoke(\"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also invoke the chain with a dictionary, this is useful when you have multiple placeholders\n",
    "capital_bot.invoke({\"country\": \"France\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Create a translator bot, that takes as an input a language and a sentence and translates it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding information to the prompt\n",
    "One way to add new information would be to add it to the prompt. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "qa_bot = prompt | gpt4o_mini | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_bot.invoke({\"context\": \"The capital of Erickland is Santillan.\", \n",
    "               \"question\": \"What is the capital of Erickland?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy because it was small. But this could potentially help by passing a whole document to the context.\n",
    "\n",
    "The code below reads NNF annual report and stores in the variables `context`. Can you do a NNF report bot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"docs/strategy.txt\", \"r\") as f:\n",
    "    context = f.read()\n",
    "\n",
    "# Printing the first 1000 characters of the context\n",
    "print(context[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a bot for the annual report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"docs/2023-annual-report.txt\", \"r\") as f:\n",
    "    annual_report = f.read()\n",
    "\n",
    "# Printing the first 1000 characters of the annual report\n",
    "print(annual_report[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "nnf_2023_bot = prompt | gpt4o_mini | StrOutputParser()\n",
    "\n",
    "nnf_2023_bot.invoke({\"context\": annual_report,\n",
    "                        \"question\": \"How many DKK in grants have been awarded by NNF in 2019-2023?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "How many words are there in the annual_report? (you can use the `len` function to count the words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n",
    "Our loaded documents seems to be too long, which means they can't fit fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
    "\n",
    "To handle this we’ll split the Document into chunks and just put in the context relevant information.\n",
    "\n",
    "In this case we’ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n",
    "\n",
    "## How to find relevant information?\n",
    "This is THE question and is a bit outside the scope of this workshop, but you can search for the key words: Information retrieval and embeddings if you are really curious.\n",
    "\n",
    "I'll provide a big part of the code that you can just re-use. Don't worry if you don't get everything :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the documents\n",
    "loader = DirectoryLoader(\"./docs/\", loader_cls=TextLoader, glob=\"*.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "#Splitting the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Creating embedding for the documents and putting them in a vector store\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding=embedding_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a retriever out of our vector store. The `6` you see is the amount of similar \"documents\" that will be retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many DKK in grants have been awarded by NNF in 2019-2023?\"\n",
    "retrieved_docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's happening under the hood? (only if you're interested)\n",
    " It may seem really obscur what one line of code is doing but it's really simple. It's a 4 step process:\n",
    " 1. The `query` is passed through our embedding model and gets transformed into a vector, let's called it `query_vector`\n",
    " 2. The `query_vector` is then compared to all the vectors in the vectorstore. Remember that those vectors in the vectorstore are just a mathematical representation of parts of the documents\n",
    " 3. We then take the vectors that are the most \"similar\" to our `query_vector`\n",
    " 4. We return a list with the documents that had the nearest distance to the `query`\n",
    " 5. We then print the first document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything together\n",
    "\n",
    "We will create a chain called rag_chain that will have only one input: the user's question.\n",
    "\n",
    "The question be forked and passed through two different pipelines:\n",
    "\n",
    "1. The retrieval pipeline, where the question will be compared to the documents inside the vectorstore using the retriever and its output will be appended usint the format_docs function. The output of this chain will be a string and be passed to prompt on the context property.\n",
    "2. The question will be other property passed to the prompt.\n",
    "Once the prompt is filled with context and the question, we will send it to the llm, and we will print out the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can safely ignore this function, it's just for formatting the output\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"How many DKK in grants have been awarded by NNF in 2019-2023?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn!\n",
    "Now it's up to you, here we propose some exercises for you to play with, feel free to mess around with it :)\n",
    "\n",
    "# Exercise 1: Validation\n",
    "Right now our agent can answer questions about PNNFlanday... but also about coding in python, or about the weather in Mexico. I think you can see how this can be abused... How can you put some guard rails to avoid it?\n",
    "\n",
    "The following prompt shouldn't be possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag_chain.invoke(\"Write a python function that somes all fibonacci numbers between 1-18\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Cite your sources!\n",
    "We know LLMs are prompt to hallucinate... how can you make it return the sources of where the knowledge came from?\n",
    "\n",
    "Pssst: maybe you want to look into modifyin the format_docs function, although there are several ways of doing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Follow-up questions *really hard*\n",
    "Right now, our agent can answer questions about NNF. But if you ask a follow up question, it has no idea about what you were talking about as an LLM has no memory. The only way to provide it with memory is by somehow adding the past requests manually to the request. How could you do it...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
